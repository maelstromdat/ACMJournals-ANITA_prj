\subsection{Answering our Research Questions}

%\begin{itemize}
%\item address each RQ individually (e.g., in a subsubsection?) by stating the RQ and then provide a summary of a response inside a framed box. with a practical impact discussion and target practitioner from academia and elsewhere
%\item offer an example use of the results in the context of each RQ
%\end{itemize}

\subsubsection{What online depth levels are assessed and to what extent?}

\subsubsection{What degrees of anonymity exist for web-crawling?}

\subsubsection{What policies exist to vary the degrees of anonymity?}

\subsubsection{What website features are most indicative of cyberthreats?}

\subsubsection{What risk assessment techniques exist?}




\subsection{Observations and Lessons Learned}

\begin{itemize}
\item propose a risk-assessment metric?
\item discuss the criminal activity types which were reported in more recent literature?
\item elaborate on crime-specific prediction as well as the emerging criminality types from above?
\item discuss the sparse community working on the topics and the missed convergence between grey and white literature --- offer a descriptive statistic over the community types emerging in the sample and where each result was published to discuss it further.
\end{itemize}

\subsection{Limitations and Threats to Validity}

Based on the taxonomy in \textcolor{red}{\cite{wohlin}, }there are four potential validity threat areas, namely: external, construct, internal, and conclusion validity. 

\emph{ ``External Validity''} concerns the applicability of the results in a more general context. Since our primary studies are obtained from a large extent of disciplines, our results and observations might be only partially applicable to cyber threat intelligence disciplines, this may threaten external validity. To strengthen external-validity, we organized feedback sessions. We analyzed follow-up discussions and used this qualitative data to fine-tune our research methods, and applicability of our results. In addition, we prepared a bundle of all the raw data, all models drawn, all tables, and everything that we used to compose this paper so as to make it available to all who might want to further their understanding on our data (for links see the Appendix). We hope that this can help in making the results and our observations more explicit and applicable in practice.

%Finally,\emph{ ``Instrumental Validity''} concerns the validity of the instruments and methods that were used to conduct the study. Instrumental validity may be threatened since our approach to grounded theory is a hybrid of two methods and might not retain the validity of either. To mitigate these threats we adopted the following counter-measures. To strengthen instrumental-validity, we consulted with grounded theory practitioners and received positive feedback on our method. Also, we consulted additional literature on grounded theory \cite{gtmeth,gtval}. We discovered other hybrids in grounded theory with few claims of invalidation, therefore we assume that our work is as much valid in this sense as others.

\emph{``Construct Validity''} and \emph{``Internal Validity''} concern the generalizability of the constructs under study, as well as the methods used to study and analyze data (e.g. the types of bias involved). To mitigate these threats, we adopted a mixed-methods research approach. On one hand, a formal grounded-theory method was conceived to avoid bias by construction\cite{straussian,gt,gtmeth}. On the other hand, we adopted machine-learning and topic modelling techniques which were appropriately fine-tuned using state of the art approaches to the purpose of elaborating the expected results. As previously explained, to ensure internal and construct validity even further, the initial set of codes for grounded-theory was developed by an external researcher and checked against another external reviewer which is not among the authors and not belonging to the software engineering field. In addition we applied grounded-theory in two rounds: (a) first the primary studies were split across a group of students, to apply grounded theory; (b) in the second round one of the authors re-executed a blind grounded-theory on the full primary studies set. When both rounds were finished, both grounded-theories were analyzed evenly to construct a unique theory. When disagreement between the two samples was found, a session was organized with students researchers and supervisors to examine the samples and check them against literature.

%from which were designed to reduce author bias to a minimum. Moreover, the coding and initial theory generation were carried out twice: first by a group of Masters' Students and then by one of the authors from scratch. Both samples were used evenly to generate a theory. Finally the theory was checked by two supervising researchers.

\emph{``Conclusion Validity''} concerns the degree to which our conclusions are reasonable based on our data. The logical reasoning behind our conclusions are dictated by sound analysis of the data through grounded theory and other analysis methods which try and construct theory from data rather than confirming initial hypotheses, as explained in \cite{gtmeth,gtval}. Moreover, all the conclusions in this paper were drawn by three researchers and double-checked against data, primary papers or related studies. 